{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 108995856.0000 - mae: 108995856.0000 - val_loss: 105278080.0000 - val_mae: 105278080.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 110585112.0000 - mae: 110585112.0000 - val_loss: 91274040.0000 - val_mae: 91274040.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 91490784.0000 - mae: 91490784.0000 - val_loss: 63825860.0000 - val_mae: 63825860.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 67787016.0000 - mae: 67787016.0000 - val_loss: 44467644.0000 - val_mae: 44467644.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 47031984.0000 - mae: 47031984.0000 - val_loss: 37819892.0000 - val_mae: 37819892.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 43534640.0000 - mae: 43534640.0000 - val_loss: 34047348.0000 - val_mae: 34047348.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - loss: 37702496.0000 - mae: 37702496.0000 - val_loss: 31632256.0000 - val_mae: 31632256.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 36657960.0000 - mae: 36657960.0000 - val_loss: 29691098.0000 - val_mae: 29691098.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 32515062.0000 - mae: 32515062.0000 - val_loss: 27978412.0000 - val_mae: 27978412.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 30932202.0000 - mae: 30932202.0000 - val_loss: 26796456.0000 - val_mae: 26796456.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 29831806.0000 - mae: 29831806.0000 - val_loss: 26011058.0000 - val_mae: 26011058.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 31026366.0000 - mae: 31026366.0000 - val_loss: 25046278.0000 - val_mae: 25046278.0000\n",
      "Epoch 13/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 30156326.0000 - mae: 30156326.0000 - val_loss: 24402446.0000 - val_mae: 24402446.0000\n",
      "Epoch 14/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 29464716.0000 - mae: 29464716.0000 - val_loss: 24259738.0000 - val_mae: 24259738.0000\n",
      "Epoch 15/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 30097034.0000 - mae: 30097034.0000 - val_loss: 23776968.0000 - val_mae: 23776968.0000\n",
      "Epoch 16/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 27556338.0000 - mae: 27556338.0000 - val_loss: 23794790.0000 - val_mae: 23794790.0000\n",
      "Epoch 17/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 27624752.0000 - mae: 27624752.0000 - val_loss: 23911970.0000 - val_mae: 23911970.0000\n",
      "Epoch 18/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 27654050.0000 - mae: 27654050.0000 - val_loss: 23530466.0000 - val_mae: 23530466.0000\n",
      "Epoch 19/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 28422000.0000 - mae: 28422000.0000 - val_loss: 23412370.0000 - val_mae: 23412370.0000\n",
      "Epoch 20/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 27555168.0000 - mae: 27555168.0000 - val_loss: 23366470.0000 - val_mae: 23366470.0000\n",
      "Epoch 21/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 27454116.0000 - mae: 27454116.0000 - val_loss: 23601490.0000 - val_mae: 23601490.0000\n",
      "Epoch 22/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 27858502.0000 - mae: 27858502.0000 - val_loss: 23268082.0000 - val_mae: 23268082.0000\n",
      "Epoch 23/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 29145360.0000 - mae: 29145360.0000 - val_loss: 23174460.0000 - val_mae: 23174460.0000\n",
      "Epoch 24/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 27982164.0000 - mae: 27982164.0000 - val_loss: 23192482.0000 - val_mae: 23192482.0000\n",
      "Epoch 25/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 27488926.0000 - mae: 27488926.0000 - val_loss: 23714488.0000 - val_mae: 23714488.0000\n",
      "Epoch 26/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 29242382.0000 - mae: 29242382.0000 - val_loss: 23222642.0000 - val_mae: 23222642.0000\n",
      "Epoch 27/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 28502828.0000 - mae: 28502828.0000 - val_loss: 23076114.0000 - val_mae: 23076114.0000\n",
      "Epoch 28/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 28533494.0000 - mae: 28533494.0000 - val_loss: 23807208.0000 - val_mae: 23807208.0000\n",
      "Epoch 29/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 27581468.0000 - mae: 27581468.0000 - val_loss: 23493992.0000 - val_mae: 23493992.0000\n",
      "Epoch 30/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 29727258.0000 - mae: 29727258.0000 - val_loss: 23081346.0000 - val_mae: 23081346.0000\n",
      "Epoch 31/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 28312950.0000 - mae: 28312950.0000 - val_loss: 23708074.0000 - val_mae: 23708074.0000\n",
      "Epoch 32/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 27109434.0000 - mae: 27109434.0000 - val_loss: 23216766.0000 - val_mae: 23216766.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 27133888.0000 - mae: 27133888.0000 - val_loss: 23536600.0000 - val_mae: 23536600.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 27749024.0000 - mae: 27749024.0000 - val_loss: 23160088.0000 - val_mae: 23160088.0000\n",
      "Epoch 35/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 28224506.0000 - mae: 28224506.0000 - val_loss: 23288450.0000 - val_mae: 23288450.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 839us/step - loss: 26446500.0000 - mae: 26446500.0000 - val_loss: 23982310.0000 - val_mae: 23982310.0000\n",
      "Epoch 37/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 25450616.0000 - mae: 25450616.0000 - val_loss: 23204222.0000 - val_mae: 23204222.0000\n",
      "Epoch 38/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 28482600.0000 - mae: 28482600.0000 - val_loss: 23606514.0000 - val_mae: 23606514.0000\n",
      "Epoch 39/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 30084754.0000 - mae: 30084754.0000 - val_loss: 23681070.0000 - val_mae: 23681070.0000\n",
      "Epoch 40/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 26802422.0000 - mae: 26802422.0000 - val_loss: 23123046.0000 - val_mae: 23123046.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 25899528.0000 - mae: 25899528.0000 - val_loss: 23419386.0000 - val_mae: 23419386.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 28272434.0000 - mae: 28272434.0000 - val_loss: 23849212.0000 - val_mae: 23849212.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 28357750.0000 - mae: 28357750.0000 - val_loss: 23136816.0000 - val_mae: 23136816.0000\n",
      "Epoch 44/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 26271934.0000 - mae: 26271934.0000 - val_loss: 23149050.0000 - val_mae: 23149050.0000\n",
      "Epoch 45/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 26974706.0000 - mae: 26974706.0000 - val_loss: 23474024.0000 - val_mae: 23474024.0000\n",
      "Epoch 46/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 28370574.0000 - mae: 28370574.0000 - val_loss: 23193614.0000 - val_mae: 23193614.0000\n",
      "Epoch 47/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 27393172.0000 - mae: 27393172.0000 - val_loss: 23261898.0000 - val_mae: 23261898.0000\n",
      "Epoch 48/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 29836572.0000 - mae: 29836572.0000 - val_loss: 23458252.0000 - val_mae: 23458252.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 27866146.0000 - mae: 27866146.0000 - val_loss: 23516612.0000 - val_mae: 23516612.0000\n",
      "Epoch 50/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 27265312.0000 - mae: 27265312.0000 - val_loss: 23343480.0000 - val_mae: 23343480.0000\n",
      "Epoch 51/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 25864124.0000 - mae: 25864124.0000 - val_loss: 23670470.0000 - val_mae: 23670470.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 28426618.0000 - mae: 28426618.0000 - val_loss: 23312872.0000 - val_mae: 23312872.0000\n",
      "Epoch 53/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 28391780.0000 - mae: 28391780.0000 - val_loss: 23249336.0000 - val_mae: 23249336.0000\n",
      "Epoch 54/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 26065620.0000 - mae: 26065620.0000 - val_loss: 23289264.0000 - val_mae: 23289264.0000\n",
      "Epoch 55/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 26244776.0000 - mae: 26244776.0000 - val_loss: 23304780.0000 - val_mae: 23304780.0000\n",
      "Epoch 56/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 28775802.0000 - mae: 28775802.0000 - val_loss: 23578588.0000 - val_mae: 23578588.0000\n",
      "Epoch 57/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 27194286.0000 - mae: 27194286.0000 - val_loss: 23379912.0000 - val_mae: 23379912.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 838us/step - loss: 25258332.0000 - mae: 25258332.0000 - val_loss: 23784456.0000 - val_mae: 23784456.0000\n",
      "Epoch 59/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 29185536.0000 - mae: 29185536.0000 - val_loss: 23304030.0000 - val_mae: 23304030.0000\n",
      "Epoch 60/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 27218880.0000 - mae: 27218880.0000 - val_loss: 23230848.0000 - val_mae: 23230848.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 27642982.0000 - mae: 27642982.0000 - val_loss: 23758966.0000 - val_mae: 23758966.0000\n",
      "Epoch 62/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 27531714.0000 - mae: 27531714.0000 - val_loss: 23253548.0000 - val_mae: 23253548.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 27718304.0000 - mae: 27718304.0000 - val_loss: 23217882.0000 - val_mae: 23217882.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 26804598.0000 - mae: 26804598.0000 - val_loss: 23399568.0000 - val_mae: 23399568.0000\n",
      "Epoch 65/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 28180462.0000 - mae: 28180462.0000 - val_loss: 23398636.0000 - val_mae: 23398636.0000\n",
      "Epoch 66/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 25973728.0000 - mae: 25973728.0000 - val_loss: 23576040.0000 - val_mae: 23576040.0000\n",
      "Epoch 67/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 26348564.0000 - mae: 26348564.0000 - val_loss: 23588434.0000 - val_mae: 23588434.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 24901758.0000 - mae: 24901758.0000 - val_loss: 23763296.0000 - val_mae: 23763296.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 29388020.0000 - mae: 29388020.0000 - val_loss: 23257450.0000 - val_mae: 23257450.0000\n",
      "Epoch 70/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 26740714.0000 - mae: 26740714.0000 - val_loss: 23392438.0000 - val_mae: 23392438.0000\n",
      "Epoch 71/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 25424038.0000 - mae: 25424038.0000 - val_loss: 23429970.0000 - val_mae: 23429970.0000\n",
      "Epoch 72/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 26813432.0000 - mae: 26813432.0000 - val_loss: 23375522.0000 - val_mae: 23375522.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 27361314.0000 - mae: 27361314.0000 - val_loss: 23468848.0000 - val_mae: 23468848.0000\n",
      "Epoch 74/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 26582986.0000 - mae: 26582986.0000 - val_loss: 23673442.0000 - val_mae: 23673442.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 27883696.0000 - mae: 27883696.0000 - val_loss: 23663328.0000 - val_mae: 23663328.0000\n",
      "Epoch 76/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 27427836.0000 - mae: 27427836.0000 - val_loss: 23715226.0000 - val_mae: 23715226.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 26928686.0000 - mae: 26928686.0000 - val_loss: 23516552.0000 - val_mae: 23516552.0000\n",
      "Epoch 78/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 27202570.0000 - mae: 27202570.0000 - val_loss: 23719304.0000 - val_mae: 23719304.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 26873138.0000 - mae: 26873138.0000 - val_loss: 24145968.0000 - val_mae: 24145968.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 28423104.0000 - mae: 28423104.0000 - val_loss: 23496088.0000 - val_mae: 23496088.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 27705604.0000 - mae: 27705604.0000 - val_loss: 23525118.0000 - val_mae: 23525118.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 26454580.0000 - mae: 26454580.0000 - val_loss: 23458878.0000 - val_mae: 23458878.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 27761242.0000 - mae: 27761242.0000 - val_loss: 23440830.0000 - val_mae: 23440830.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 25160206.0000 - mae: 25160206.0000 - val_loss: 23557986.0000 - val_mae: 23557986.0000\n",
      "Epoch 85/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 24369078.0000 - mae: 24369078.0000 - val_loss: 24017132.0000 - val_mae: 24017132.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 26331180.0000 - mae: 26331180.0000 - val_loss: 23708036.0000 - val_mae: 23708036.0000\n",
      "Epoch 87/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 26222658.0000 - mae: 26222658.0000 - val_loss: 23424174.0000 - val_mae: 23424174.0000\n",
      "Epoch 88/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 26306162.0000 - mae: 26306162.0000 - val_loss: 23500360.0000 - val_mae: 23500360.0000\n",
      "Epoch 89/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 26038494.0000 - mae: 26038494.0000 - val_loss: 24350902.0000 - val_mae: 24350900.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - loss: 27766634.0000 - mae: 27766634.0000 - val_loss: 23430686.0000 - val_mae: 23430686.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 27619376.0000 - mae: 27619376.0000 - val_loss: 23377794.0000 - val_mae: 23377794.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 25737934.0000 - mae: 25737934.0000 - val_loss: 23286504.0000 - val_mae: 23286504.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 26438926.0000 - mae: 26438926.0000 - val_loss: 24005268.0000 - val_mae: 24005268.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 25585052.0000 - mae: 25585052.0000 - val_loss: 23516676.0000 - val_mae: 23516676.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 25453666.0000 - mae: 25453666.0000 - val_loss: 23630254.0000 - val_mae: 23630254.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 26351624.0000 - mae: 26351624.0000 - val_loss: 23465940.0000 - val_mae: 23465940.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 28516378.0000 - mae: 28516378.0000 - val_loss: 23494566.0000 - val_mae: 23494566.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 26098574.0000 - mae: 26098574.0000 - val_loss: 23474184.0000 - val_mae: 23474184.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 26946556.0000 - mae: 26946556.0000 - val_loss: 23910532.0000 - val_mae: 23910532.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27706140.0000 - mae: 27706140.0000 - val_loss: 23509452.0000 - val_mae: 23509452.0000\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335us/step - loss: 29453884.0000 - mae: 29453884.0000\n",
      "Mean Absolute Error on Test Set: 28204438.0\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos\n",
    "movies_df = pd.read_csv(\"data/movie_statistic_dataset.csv\")\n",
    "\n",
    "# Selección de columnas relevantes\n",
    "relevant_columns = [\n",
    "    \"production_date\",\n",
    "    \"genres\",\n",
    "    \"runtime_minutes\",\n",
    "    # \"director_name\",\n",
    "    \"movie_averageRating\",\n",
    "    \"movie_numberOfVotes\",\n",
    "    \"approval_Index\",\n",
    "    \"Production budget $\",\n",
    "    \"Domestic gross $\",\n",
    "    \"Worldwide gross $\",\n",
    "]\n",
    "movies_df = movies_df[relevant_columns]\n",
    "\n",
    "movies_df = movies_df.dropna()\n",
    "\n",
    "movies_df[\"production_date\"] = pd.to_datetime(movies_df[\"production_date\"])\n",
    "\n",
    "\n",
    "movies_df[\"production_year\"] = movies_df[\"production_date\"].dt.year\n",
    "movies_df[\"production_month\"] = movies_df[\"production_date\"].dt.month\n",
    "movies_df[\"production_day\"] = movies_df[\"production_date\"].dt.day\n",
    "\n",
    "movies_df = movies_df.drop(columns=[\"production_date\"])\n",
    "\n",
    "# Split the 'genres' column into three separate columns\n",
    "def split_genres(genres):\n",
    "    genres_list = genres.split(\",\") if isinstance(genres, str) else []\n",
    "    return pd.Series(genres_list[:3] + [None] * (3 - len(genres_list)))\n",
    "\n",
    "\n",
    "movies_df[[\"genre_1\", \"genre_2\", \"genre_3\"]] = movies_df[\"genres\"].apply(split_genres)\n",
    "\n",
    "# # Drop the original 'genres' column\n",
    "movies_df = movies_df.drop(columns=[\"genres\"])\n",
    "\n",
    "# Separación de características y variable objetivo\n",
    "X = movies_df.drop(columns=[\"Worldwide gross $\"])\n",
    "y = movies_df[\"Worldwide gross $\"]\n",
    "\n",
    "# Preprocesamiento de datos categóricos y numéricos\n",
    "categorical_features = [\"genre_1\", \"genre_2\", \"genre_3\"]\n",
    "numeric_features = [\n",
    "    \"runtime_minutes\",\n",
    "    \"movie_averageRating\",\n",
    "    \"movie_numberOfVotes\",\n",
    "    \"approval_Index\",\n",
    "    \"Production budget $\",\n",
    "    \"Domestic gross $\",\n",
    "    \"production_year\",\n",
    "    \"production_month\",\n",
    "    \"production_day\",\n",
    "]\n",
    "\n",
    "# One-Hot Encoding for categorical features and Standard Scaling for numerical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor)])\n",
    "\n",
    "\n",
    "X_train_processed = pipeline.fit_transform(X_train)\n",
    "X_test_processed = pipeline.transform(X_test)\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(\n",
    "            32,\n",
    "            activation=\"relu\",\n",
    "            input_shape=(X_train_processed.shape[1],),\n",
    "        ),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(1),  # Capa de salida para regresión\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.004), loss=\"mean_absolute_error\", metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_processed, y_train, epochs=100, batch_size=32, validation_split=0.2\n",
    ")\n",
    "loss, mae = model.evaluate(X_test_processed, y_test)\n",
    "print(f\"Mean Absolute Error on Test Set: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    val_metrics = history.history['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics)\n",
    "    plt.plot(epochs, val_metrics)\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"408.10125pt\" height=\"325.986375pt\" viewBox=\"0 0 408.10125 325.986375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-08-05T16:21:27.908007</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 325.986375 \n",
       "L 408.10125 325.986375 \n",
       "L 408.10125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 43.78125 288.430125 \n",
       "L 400.90125 288.430125 \n",
       "L 400.90125 22.318125 \n",
       "L 43.78125 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m649478a82d\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m649478a82d\" x=\"56.734638\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(53.553388 303.028562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m649478a82d\" x=\"122.321415\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(115.958915 303.028562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m649478a82d\" x=\"187.908192\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(181.545692 303.028562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m649478a82d\" x=\"253.494969\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 60 -->\n",
       "      <g transform=\"translate(247.132469 303.028562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m649478a82d\" x=\"319.081746\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 80 -->\n",
       "      <g transform=\"translate(312.719246 303.028562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m649478a82d\" x=\"384.668523\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(375.124773 303.028562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- Epochs -->\n",
       "     <g transform=\"translate(204.425625 316.706687) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-45\" d=\"M 628 4666 \n",
       "L 3578 4666 \n",
       "L 3578 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2753 \n",
       "L 3481 2753 \n",
       "L 3481 2222 \n",
       "L 1259 2222 \n",
       "L 1259 531 \n",
       "L 3634 531 \n",
       "L 3634 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-45\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"63.183594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"126.660156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"187.841797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"242.822266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"306.201172\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"mb05df370ad\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb05df370ad\" x=\"43.78125\" y=\"284.583847\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(20.878125 288.383066) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb05df370ad\" x=\"43.78125\" y=\"230.946549\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(20.878125 234.745768) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb05df370ad\" x=\"43.78125\" y=\"177.309251\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(20.878125 181.10847) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb05df370ad\" x=\"43.78125\" y=\"123.671953\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(20.878125 127.471172) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb05df370ad\" x=\"43.78125\" y=\"70.034655\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(20.878125 73.833873) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- mae -->\n",
       "     <g transform=\"translate(14.798438 166.385062) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"97.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"158.691406\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- 1e8 -->\n",
       "     <g transform=\"translate(43.78125 19.318125) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"63.623047\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-38\" x=\"125.146484\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_12\">\n",
       "    <path d=\"M 60.013977 34.414125 \n",
       "L 63.293316 46.761688 \n",
       "L 66.572655 108.103148 \n",
       "L 69.851994 180.648773 \n",
       "L 73.131333 213.186596 \n",
       "L 76.410671 228.155307 \n",
       "L 79.69001 237.195208 \n",
       "L 82.969349 243.331004 \n",
       "L 86.248688 246.913987 \n",
       "L 89.528027 252.349681 \n",
       "L 92.807366 255.202993 \n",
       "L 96.086705 254.48708 \n",
       "L 99.366043 256.978693 \n",
       "L 102.645382 260.562271 \n",
       "L 105.924721 261.155966 \n",
       "L 109.20406 262.170349 \n",
       "L 112.483399 260.053199 \n",
       "L 115.762738 260.900534 \n",
       "L 119.042076 260.475802 \n",
       "L 122.321415 260.532314 \n",
       "L 125.600754 263.086447 \n",
       "L 128.880093 263.091339 \n",
       "L 132.159432 262.89279 \n",
       "L 135.438771 262.403065 \n",
       "L 138.71811 263.168566 \n",
       "L 141.997448 263.464676 \n",
       "L 145.276787 262.641386 \n",
       "L 148.556126 264.199512 \n",
       "L 151.835465 263.922642 \n",
       "L 155.114804 261.265069 \n",
       "L 158.394143 265.326158 \n",
       "L 161.673481 263.108401 \n",
       "L 164.95282 261.633493 \n",
       "L 168.232159 263.06931 \n",
       "L 171.511498 262.760869 \n",
       "L 174.790837 264.270737 \n",
       "L 178.070176 265.550067 \n",
       "L 181.349514 263.926649 \n",
       "L 184.628853 262.626143 \n",
       "L 187.908192 265.192929 \n",
       "L 191.187531 265.475136 \n",
       "L 194.46687 265.140375 \n",
       "L 197.746209 263.669994 \n",
       "L 201.025548 265.635485 \n",
       "L 204.304886 264.145537 \n",
       "L 207.584225 263.495389 \n",
       "L 210.863564 263.007515 \n",
       "L 214.142903 263.55196 \n",
       "L 217.422242 264.936006 \n",
       "L 220.701581 265.815272 \n",
       "L 223.980919 265.22996 \n",
       "L 227.260258 264.716667 \n",
       "L 230.539597 264.34119 \n",
       "L 233.818936 265.945176 \n",
       "L 237.098275 265.76659 \n",
       "L 240.377614 266.696253 \n",
       "L 243.656952 264.814019 \n",
       "L 246.936291 265.715377 \n",
       "L 250.21563 262.372519 \n",
       "L 253.494969 265.180549 \n",
       "L 256.774308 264.040156 \n",
       "L 260.053647 265.838893 \n",
       "L 263.332986 264.736459 \n",
       "L 266.612324 265.817245 \n",
       "L 269.891663 264.98982 \n",
       "L 273.171002 267.530115 \n",
       "L 276.450341 265.495159 \n",
       "L 279.72968 266.482981 \n",
       "L 283.009019 264.395138 \n",
       "L 286.288357 266.391025 \n",
       "L 289.567696 265.578329 \n",
       "L 292.847035 264.572082 \n",
       "L 296.126374 267.743924 \n",
       "L 299.405713 267.755333 \n",
       "L 302.685052 265.23711 \n",
       "L 305.96439 266.182102 \n",
       "L 309.243729 266.713975 \n",
       "L 312.523068 265.672511 \n",
       "L 315.802407 266.524464 \n",
       "L 319.081746 264.791175 \n",
       "L 322.361085 263.491076 \n",
       "L 325.640424 266.414524 \n",
       "L 328.919762 264.362285 \n",
       "L 332.199101 266.104366 \n",
       "L 335.47844 266.862127 \n",
       "L 338.757779 268.382068 \n",
       "L 342.037118 267.263859 \n",
       "L 345.316457 267.683443 \n",
       "L 348.595795 267.59972 \n",
       "L 351.875134 266.398588 \n",
       "L 355.154473 265.610753 \n",
       "L 358.433812 266.366084 \n",
       "L 361.713151 266.728409 \n",
       "L 364.99249 268.14246 \n",
       "L 368.271829 266.554211 \n",
       "L 371.551167 263.602824 \n",
       "L 374.830506 265.198303 \n",
       "L 378.109845 266.81005 \n",
       "L 381.389184 265.71151 \n",
       "L 384.668523 266.366298 \n",
       "\" clip-path=\"url(#p2a60d21cfb)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_13\">\n",
       "    <path d=\"M 60.013977 55.879557 \n",
       "L 63.293316 93.436501 \n",
       "L 66.572655 167.048811 \n",
       "L 69.851994 218.964931 \n",
       "L 73.131333 236.793304 \n",
       "L 76.410671 246.910758 \n",
       "L 79.69001 253.387708 \n",
       "L 82.969349 258.593632 \n",
       "L 86.248688 263.186824 \n",
       "L 89.528027 266.35667 \n",
       "L 92.807366 268.463002 \n",
       "L 96.086705 271.050411 \n",
       "L 99.366043 272.777082 \n",
       "L 102.645382 273.159805 \n",
       "L 105.924721 274.454529 \n",
       "L 109.20406 274.406733 \n",
       "L 112.483399 274.092472 \n",
       "L 115.762738 275.115614 \n",
       "L 119.042076 275.432332 \n",
       "L 122.321415 275.555429 \n",
       "L 125.600754 274.925138 \n",
       "L 128.880093 275.819293 \n",
       "L 132.159432 276.070374 \n",
       "L 135.438771 276.022042 \n",
       "L 138.71811 274.622092 \n",
       "L 141.997448 275.941157 \n",
       "L 145.276787 276.334125 \n",
       "L 148.556126 274.37343 \n",
       "L 151.835465 275.213433 \n",
       "L 155.114804 276.320093 \n",
       "L 158.394143 274.639294 \n",
       "L 161.673481 275.956915 \n",
       "L 164.95282 275.099164 \n",
       "L 168.232159 276.108918 \n",
       "L 171.511498 275.764669 \n",
       "L 174.790837 273.90383 \n",
       "L 178.070176 275.990557 \n",
       "L 181.349514 274.911664 \n",
       "L 184.628853 274.711715 \n",
       "L 187.908192 276.20826 \n",
       "L 191.187531 275.413516 \n",
       "L 194.46687 274.260781 \n",
       "L 197.746209 276.17133 \n",
       "L 201.025548 276.138521 \n",
       "L 204.304886 275.266984 \n",
       "L 207.584225 276.019006 \n",
       "L 210.863564 275.835877 \n",
       "L 214.142903 275.309283 \n",
       "L 217.422242 275.152769 \n",
       "L 220.701581 275.617086 \n",
       "L 223.980919 274.740143 \n",
       "L 227.260258 275.699172 \n",
       "L 230.539597 275.869567 \n",
       "L 233.818936 275.762485 \n",
       "L 237.098275 275.720874 \n",
       "L 240.377614 274.986558 \n",
       "L 243.656952 275.51938 \n",
       "L 246.936291 274.434447 \n",
       "L 250.21563 275.722885 \n",
       "L 253.494969 275.919149 \n",
       "L 256.774308 274.502808 \n",
       "L 260.053647 275.858271 \n",
       "L 263.332986 275.953922 \n",
       "L 266.612324 275.466665 \n",
       "L 269.891663 275.469165 \n",
       "L 273.171002 274.993391 \n",
       "L 276.450341 274.960152 \n",
       "L 279.72968 274.491196 \n",
       "L 283.009019 275.847806 \n",
       "L 286.288357 275.485787 \n",
       "L 289.567696 275.385131 \n",
       "L 292.847035 275.531153 \n",
       "L 296.126374 275.280865 \n",
       "L 299.405713 274.732172 \n",
       "L 302.685052 274.759296 \n",
       "L 305.96439 274.620113 \n",
       "L 309.243729 275.15293 \n",
       "L 312.523068 274.609176 \n",
       "L 315.802407 273.464921 \n",
       "L 319.081746 275.207811 \n",
       "L 322.361085 275.129957 \n",
       "L 325.640424 275.307604 \n",
       "L 328.919762 275.356006 \n",
       "L 332.199101 275.041809 \n",
       "L 335.47844 273.810442 \n",
       "L 338.757779 274.639396 \n",
       "L 342.037118 275.400675 \n",
       "L 345.316457 275.196355 \n",
       "L 348.595795 272.915321 \n",
       "L 351.875134 275.383211 \n",
       "L 355.154473 275.52506 \n",
       "L 358.433812 275.769887 \n",
       "L 361.713151 273.842259 \n",
       "L 364.99249 275.152597 \n",
       "L 368.271829 274.847996 \n",
       "L 371.551167 275.288664 \n",
       "L 374.830506 275.211893 \n",
       "L 378.109845 275.266555 \n",
       "L 381.389184 274.096329 \n",
       "L 384.668523 275.171971 \n",
       "\" clip-path=\"url(#p2a60d21cfb)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 43.78125 288.430125 \n",
       "L 43.78125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 400.90125 288.430125 \n",
       "L 400.90125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 43.78125 288.430125 \n",
       "L 400.90125 288.430125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 43.78125 22.318125 \n",
       "L 400.90125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_15\">\n",
       "    <!-- Training and validation mae -->\n",
       "    <g transform=\"translate(138.9075 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-54\" d=\"M -19 4666 \n",
       "L 3928 4666 \n",
       "L 3928 4134 \n",
       "L 2272 4134 \n",
       "L 2272 0 \n",
       "L 1638 0 \n",
       "L 1638 4134 \n",
       "L -19 4134 \n",
       "L -19 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"46.333984\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"87.447266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"148.726562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"176.509766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"239.888672\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"267.671875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"331.050781\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"394.527344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"426.314453\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"487.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"550.972656\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"614.449219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"646.236328\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"705.416016\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"766.695312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"794.478516\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"822.261719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"885.738281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"947.017578\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"986.226562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1014.009766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1075.191406\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1138.570312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"1170.357422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"1267.769531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1329.048828\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 311.604375 60.230625 \n",
       "L 393.90125 60.230625 \n",
       "Q 395.90125 60.230625 395.90125 58.230625 \n",
       "L 395.90125 29.318125 \n",
       "Q 395.90125 27.318125 393.90125 27.318125 \n",
       "L 311.604375 27.318125 \n",
       "Q 309.604375 27.318125 309.604375 29.318125 \n",
       "L 309.604375 58.230625 \n",
       "Q 309.604375 60.230625 311.604375 60.230625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_14\">\n",
       "     <path d=\"M 313.604375 35.416562 \n",
       "L 323.604375 35.416562 \n",
       "L 333.604375 35.416562 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_16\">\n",
       "     <!-- train_mae -->\n",
       "     <g transform=\"translate(341.604375 38.916562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \n",
       "L 3263 -1509 \n",
       "L -63 -1509 \n",
       "L -63 -1063 \n",
       "L 3263 -1063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"380.175781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"441.455078\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_15\">\n",
       "     <path d=\"M 313.604375 50.372813 \n",
       "L 323.604375 50.372813 \n",
       "L 333.604375 50.372813 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- val_mae -->\n",
       "     <g transform=\"translate(341.604375 53.872813) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"295.654297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"356.933594\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p2a60d21cfb\">\n",
       "   <rect x=\"43.78125\" y=\"22.318125\" width=\"357.12\" height=\"266.112\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metric(history, \"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 113503744.0000 - mae: 113503744.0000 - val_loss: 104459736.0000 - val_mae: 104459736.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 108599704.0000 - mae: 108599704.0000 - val_loss: 74610448.0000 - val_mae: 74610448.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 74815016.0000 - mae: 74815016.0000 - val_loss: 45549152.0000 - val_mae: 45549152.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 50034964.0000 - mae: 50034964.0000 - val_loss: 36821128.0000 - val_mae: 36821128.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 39389552.0000 - mae: 39389552.0000 - val_loss: 32465980.0000 - val_mae: 32465980.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 41662756.0000 - mae: 41662756.0000 - val_loss: 29601932.0000 - val_mae: 29601932.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 33962108.0000 - mae: 33962108.0000 - val_loss: 27973060.0000 - val_mae: 27973060.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 35001788.0000 - mae: 35001788.0000 - val_loss: 26452254.0000 - val_mae: 26452254.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 31087294.0000 - mae: 31087294.0000 - val_loss: 25604230.0000 - val_mae: 25604230.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 29210628.0000 - mae: 29210628.0000 - val_loss: 24540000.0000 - val_mae: 24540000.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 28263358.0000 - mae: 28263358.0000 - val_loss: 24419486.0000 - val_mae: 24419486.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 28464048.0000 - mae: 28464048.0000 - val_loss: 24085486.0000 - val_mae: 24085486.0000\n",
      "Epoch 13/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 28597014.0000 - mae: 28597014.0000 - val_loss: 24589790.0000 - val_mae: 24589790.0000\n",
      "Epoch 14/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 28056072.0000 - mae: 28056072.0000 - val_loss: 23738256.0000 - val_mae: 23738256.0000\n",
      "Epoch 15/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 27429514.0000 - mae: 27429514.0000 - val_loss: 23465474.0000 - val_mae: 23465474.0000\n",
      "Epoch 16/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 27235988.0000 - mae: 27235988.0000 - val_loss: 23763602.0000 - val_mae: 23763602.0000\n",
      "Epoch 17/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 28218426.0000 - mae: 28218426.0000 - val_loss: 23480276.0000 - val_mae: 23480276.0000\n",
      "Epoch 18/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 27389906.0000 - mae: 27389906.0000 - val_loss: 23207968.0000 - val_mae: 23207968.0000\n",
      "Epoch 19/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 29173858.0000 - mae: 29173858.0000 - val_loss: 23228170.0000 - val_mae: 23228170.0000\n",
      "Epoch 20/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 30066384.0000 - mae: 30066384.0000 - val_loss: 23430994.0000 - val_mae: 23430994.0000\n",
      "Epoch 21/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 26854470.0000 - mae: 26854470.0000 - val_loss: 23391590.0000 - val_mae: 23391590.0000\n",
      "Epoch 22/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 26335152.0000 - mae: 26335152.0000 - val_loss: 23789772.0000 - val_mae: 23789772.0000\n",
      "Epoch 23/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 25469990.0000 - mae: 25469990.0000 - val_loss: 23547910.0000 - val_mae: 23547910.0000\n",
      "Epoch 24/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 29580862.0000 - mae: 29580862.0000 - val_loss: 23401304.0000 - val_mae: 23401304.0000\n",
      "Epoch 25/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 27179550.0000 - mae: 27179550.0000 - val_loss: 23395282.0000 - val_mae: 23395282.0000\n",
      "Epoch 26/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 26490948.0000 - mae: 26490948.0000 - val_loss: 23541584.0000 - val_mae: 23541584.0000\n",
      "Epoch 27/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 27965206.0000 - mae: 27965206.0000 - val_loss: 23281482.0000 - val_mae: 23281482.0000\n",
      "Epoch 28/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 25794300.0000 - mae: 25794300.0000 - val_loss: 23522756.0000 - val_mae: 23522756.0000\n",
      "Epoch 29/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 27451868.0000 - mae: 27451868.0000 - val_loss: 23244610.0000 - val_mae: 23244610.0000\n",
      "Epoch 30/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 27028344.0000 - mae: 27028344.0000 - val_loss: 23592040.0000 - val_mae: 23592040.0000\n",
      "Epoch 31/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 25913536.0000 - mae: 25913536.0000 - val_loss: 23291804.0000 - val_mae: 23291804.0000\n",
      "Epoch 32/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 27017774.0000 - mae: 27017774.0000 - val_loss: 23882630.0000 - val_mae: 23882630.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 25551742.0000 - mae: 25551742.0000 - val_loss: 23404666.0000 - val_mae: 23404666.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 26177822.0000 - mae: 26177822.0000 - val_loss: 23434536.0000 - val_mae: 23434536.0000\n",
      "Epoch 35/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 25794588.0000 - mae: 25794588.0000 - val_loss: 23620156.0000 - val_mae: 23620156.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 25318434.0000 - mae: 25318434.0000 - val_loss: 23888042.0000 - val_mae: 23888042.0000\n",
      "Epoch 37/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 25765858.0000 - mae: 25765858.0000 - val_loss: 23927186.0000 - val_mae: 23927186.0000\n",
      "Epoch 38/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 27599464.0000 - mae: 27599464.0000 - val_loss: 23499614.0000 - val_mae: 23499614.0000\n",
      "Epoch 39/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 25201124.0000 - mae: 25201124.0000 - val_loss: 23327324.0000 - val_mae: 23327324.0000\n",
      "Epoch 40/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 27269206.0000 - mae: 27269206.0000 - val_loss: 23360834.0000 - val_mae: 23360834.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 26193702.0000 - mae: 26193702.0000 - val_loss: 23948166.0000 - val_mae: 23948166.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 25517536.0000 - mae: 25517536.0000 - val_loss: 23519646.0000 - val_mae: 23519646.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 25520504.0000 - mae: 25520504.0000 - val_loss: 23748734.0000 - val_mae: 23748734.0000\n",
      "Epoch 44/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 26434054.0000 - mae: 26434054.0000 - val_loss: 23283746.0000 - val_mae: 23283746.0000\n",
      "Epoch 45/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 24977162.0000 - mae: 24977162.0000 - val_loss: 23743034.0000 - val_mae: 23743034.0000\n",
      "Epoch 46/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 25485296.0000 - mae: 25485296.0000 - val_loss: 23388996.0000 - val_mae: 23388996.0000\n",
      "Epoch 47/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 24833014.0000 - mae: 24833014.0000 - val_loss: 23376164.0000 - val_mae: 23376164.0000\n",
      "Epoch 48/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - loss: 24874258.0000 - mae: 24874258.0000 - val_loss: 23364628.0000 - val_mae: 23364628.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 25900080.0000 - mae: 25900080.0000 - val_loss: 23460292.0000 - val_mae: 23460292.0000\n",
      "Epoch 50/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 27254720.0000 - mae: 27254720.0000 - val_loss: 24068554.0000 - val_mae: 24068554.0000\n",
      "Epoch 51/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 24783438.0000 - mae: 24783438.0000 - val_loss: 23813450.0000 - val_mae: 23813450.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 26997148.0000 - mae: 26997148.0000 - val_loss: 23713482.0000 - val_mae: 23713482.0000\n",
      "Epoch 53/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 26487792.0000 - mae: 26487792.0000 - val_loss: 23703872.0000 - val_mae: 23703872.0000\n",
      "Epoch 54/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 26052974.0000 - mae: 26052974.0000 - val_loss: 23431728.0000 - val_mae: 23431728.0000\n",
      "Epoch 55/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 26547424.0000 - mae: 26547424.0000 - val_loss: 23705370.0000 - val_mae: 23705370.0000\n",
      "Epoch 56/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 24917168.0000 - mae: 24917168.0000 - val_loss: 23396246.0000 - val_mae: 23396246.0000\n",
      "Epoch 57/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 26144304.0000 - mae: 26144304.0000 - val_loss: 23554340.0000 - val_mae: 23554340.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 26184706.0000 - mae: 26184706.0000 - val_loss: 23552930.0000 - val_mae: 23552930.0000\n",
      "Epoch 59/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 26244130.0000 - mae: 26244130.0000 - val_loss: 23833078.0000 - val_mae: 23833078.0000\n",
      "Epoch 60/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 27477638.0000 - mae: 27477638.0000 - val_loss: 24338870.0000 - val_mae: 24338870.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 25147268.0000 - mae: 25147268.0000 - val_loss: 23351560.0000 - val_mae: 23351560.0000\n",
      "Epoch 62/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 27220502.0000 - mae: 27220502.0000 - val_loss: 23308438.0000 - val_mae: 23308438.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 25055766.0000 - mae: 25055766.0000 - val_loss: 23892274.0000 - val_mae: 23892274.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 27177720.0000 - mae: 27177720.0000 - val_loss: 23710206.0000 - val_mae: 23710206.0000\n",
      "Epoch 65/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 26247016.0000 - mae: 26247016.0000 - val_loss: 23512410.0000 - val_mae: 23512410.0000\n",
      "Epoch 66/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 26274100.0000 - mae: 26274100.0000 - val_loss: 23605400.0000 - val_mae: 23605400.0000\n",
      "Epoch 67/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 25855616.0000 - mae: 25855616.0000 - val_loss: 23393522.0000 - val_mae: 23393522.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 25756428.0000 - mae: 25756428.0000 - val_loss: 23724560.0000 - val_mae: 23724560.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 26655108.0000 - mae: 26655108.0000 - val_loss: 23838198.0000 - val_mae: 23838198.0000\n",
      "Epoch 70/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - loss: 25866956.0000 - mae: 25866956.0000 - val_loss: 23515868.0000 - val_mae: 23515868.0000\n",
      "Epoch 71/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 26588482.0000 - mae: 26588482.0000 - val_loss: 23744012.0000 - val_mae: 23744012.0000\n",
      "Epoch 72/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 25202910.0000 - mae: 25202910.0000 - val_loss: 23692764.0000 - val_mae: 23692764.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 27787126.0000 - mae: 27787126.0000 - val_loss: 23694050.0000 - val_mae: 23694050.0000\n",
      "Epoch 74/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 24911526.0000 - mae: 24911526.0000 - val_loss: 23626148.0000 - val_mae: 23626148.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 26202172.0000 - mae: 26202172.0000 - val_loss: 23656770.0000 - val_mae: 23656770.0000\n",
      "Epoch 76/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 26466990.0000 - mae: 26466990.0000 - val_loss: 23689244.0000 - val_mae: 23689244.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 26385422.0000 - mae: 26385422.0000 - val_loss: 24031254.0000 - val_mae: 24031254.0000\n",
      "Epoch 78/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 25035460.0000 - mae: 25035460.0000 - val_loss: 24018182.0000 - val_mae: 24018182.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 27055992.0000 - mae: 27055992.0000 - val_loss: 23473476.0000 - val_mae: 23473476.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 27421684.0000 - mae: 27421684.0000 - val_loss: 23439076.0000 - val_mae: 23439076.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 25254600.0000 - mae: 25254600.0000 - val_loss: 23561686.0000 - val_mae: 23561686.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 26162268.0000 - mae: 26162268.0000 - val_loss: 23533784.0000 - val_mae: 23533784.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 26137216.0000 - mae: 26137216.0000 - val_loss: 23843864.0000 - val_mae: 23843864.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - loss: 24065766.0000 - mae: 24065766.0000 - val_loss: 23729570.0000 - val_mae: 23729570.0000\n",
      "Epoch 85/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 25331268.0000 - mae: 25331268.0000 - val_loss: 23605704.0000 - val_mae: 23605704.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 26711276.0000 - mae: 26711276.0000 - val_loss: 23492090.0000 - val_mae: 23492090.0000\n",
      "Epoch 87/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 25419024.0000 - mae: 25419024.0000 - val_loss: 23454502.0000 - val_mae: 23454502.0000\n",
      "Epoch 88/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 25396278.0000 - mae: 25396278.0000 - val_loss: 23480036.0000 - val_mae: 23480036.0000\n",
      "Epoch 89/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 839us/step - loss: 25619978.0000 - mae: 25619978.0000 - val_loss: 23748236.0000 - val_mae: 23748236.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 26446014.0000 - mae: 26446014.0000 - val_loss: 23769806.0000 - val_mae: 23769806.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 24775804.0000 - mae: 24775804.0000 - val_loss: 23657264.0000 - val_mae: 23657264.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 25170080.0000 - mae: 25170080.0000 - val_loss: 23903144.0000 - val_mae: 23903144.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 24410992.0000 - mae: 24410992.0000 - val_loss: 23646330.0000 - val_mae: 23646330.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 26481374.0000 - mae: 26481374.0000 - val_loss: 23862172.0000 - val_mae: 23862172.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 27317936.0000 - mae: 27317936.0000 - val_loss: 23530342.0000 - val_mae: 23530342.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 24109464.0000 - mae: 24109464.0000 - val_loss: 23435548.0000 - val_mae: 23435548.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 23883036.0000 - mae: 23883036.0000 - val_loss: 23318980.0000 - val_mae: 23318980.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 25591316.0000 - mae: 25591316.0000 - val_loss: 23619954.0000 - val_mae: 23619954.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 26066292.0000 - mae: 26066292.0000 - val_loss: 23747842.0000 - val_mae: 23747842.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 26460252.0000 - mae: 26460252.0000 - val_loss: 24064056.0000 - val_mae: 24064056.0000\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313us/step - loss: 28608378.0000 - mae: 28608378.0000\n",
      "Mean Absolute Error on Test Set: 27972058.0\n"
     ]
    }
   ],
   "source": [
    "# según Saturn cloud, aumentar el numero de capas es el primer paso para mejorar la exactitud (accuracy del modelo)\n",
    "# https://saturncloud.io/blog/how-to-improve-accuracy-in-neural-networks-with-keras/\n",
    "model2 = Sequential(\n",
    "    [\n",
    "        Dense(\n",
    "            units=64,\n",
    "            activation=\"relu\",\n",
    "            input_shape=(X_train_processed.shape[1],),\n",
    "        ),\n",
    "        Dropout(0.2),\n",
    "        Dense(\n",
    "            units=64,\n",
    "            activation=\"relu\"\n",
    "        ),\n",
    "        Dropout(0.2),\n",
    "        Dense(\n",
    "            units=64,\n",
    "            activation=\"relu\"\n",
    "        ),\n",
    "        Dropout(0.2),\n",
    "        Dense(\n",
    "            units=64,\n",
    "            activation=\"relu\"\n",
    "        ),\n",
    "        Dense(1),  # Capa de salida para regresión\n",
    "    ]\n",
    ")\n",
    "\n",
    "model2.compile(\n",
    "    optimizer=Adam(learning_rate=0.004), loss=\"mean_absolute_error\", metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "history = model2.fit(\n",
    "    X_train_processed, y_train, epochs=100, batch_size=32, validation_split=0.2\n",
    ")\n",
    "loss, mae = model2.evaluate(X_test_processed, y_test)\n",
    "print(f\"Mean Absolute Error on Test Set: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 113851520.0000 - mae: 113851520.0000 - val_loss: 103987976.0000 - val_mae: 103987976.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 102058392.0000 - mae: 102058392.0000 - val_loss: 67137552.0000 - val_mae: 67137552.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 67694592.0000 - mae: 67694592.0000 - val_loss: 42192956.0000 - val_mae: 42192956.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 42497260.0000 - mae: 42497260.0000 - val_loss: 35950424.0000 - val_mae: 35950424.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 38100144.0000 - mae: 38100144.0000 - val_loss: 31108136.0000 - val_mae: 31108136.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - loss: 33671196.0000 - mae: 33671196.0000 - val_loss: 28645682.0000 - val_mae: 28645682.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 30954876.0000 - mae: 30954876.0000 - val_loss: 26960650.0000 - val_mae: 26960650.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - loss: 29066172.0000 - mae: 29066172.0000 - val_loss: 25598042.0000 - val_mae: 25598042.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 29670976.0000 - mae: 29670976.0000 - val_loss: 24968520.0000 - val_mae: 24968520.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - loss: 29189658.0000 - mae: 29189658.0000 - val_loss: 24891284.0000 - val_mae: 24891284.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - loss: 26538006.0000 - mae: 26538006.0000 - val_loss: 24287580.0000 - val_mae: 24287580.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 26367392.0000 - mae: 26367392.0000 - val_loss: 23779008.0000 - val_mae: 23779008.0000\n",
      "Epoch 13/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - loss: 28633856.0000 - mae: 28633856.0000 - val_loss: 23755904.0000 - val_mae: 23755904.0000\n",
      "Epoch 14/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - loss: 26001506.0000 - mae: 26001506.0000 - val_loss: 23637980.0000 - val_mae: 23637980.0000\n",
      "Epoch 15/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - loss: 28457668.0000 - mae: 28457668.0000 - val_loss: 23542784.0000 - val_mae: 23542784.0000\n",
      "Epoch 16/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - loss: 28202584.0000 - mae: 28202584.0000 - val_loss: 23486214.0000 - val_mae: 23486214.0000\n",
      "Epoch 17/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - loss: 27006222.0000 - mae: 27006222.0000 - val_loss: 23817168.0000 - val_mae: 23817168.0000\n",
      "Epoch 18/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 25173136.0000 - mae: 25173136.0000 - val_loss: 23716338.0000 - val_mae: 23716338.0000\n",
      "Epoch 19/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 26165552.0000 - mae: 26165552.0000 - val_loss: 23592702.0000 - val_mae: 23592702.0000\n",
      "Epoch 20/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 28851610.0000 - mae: 28851610.0000 - val_loss: 23814830.0000 - val_mae: 23814830.0000\n",
      "Epoch 21/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - loss: 26904958.0000 - mae: 26904958.0000 - val_loss: 23716716.0000 - val_mae: 23716716.0000\n",
      "Epoch 22/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 27311412.0000 - mae: 27311412.0000 - val_loss: 23844008.0000 - val_mae: 23844008.0000\n",
      "Epoch 23/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 24461202.0000 - mae: 24461202.0000 - val_loss: 23814740.0000 - val_mae: 23814740.0000\n",
      "Epoch 24/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 24636630.0000 - mae: 24636630.0000 - val_loss: 23599054.0000 - val_mae: 23599054.0000\n",
      "Epoch 25/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 26725430.0000 - mae: 26725430.0000 - val_loss: 23719674.0000 - val_mae: 23719674.0000\n",
      "Epoch 26/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 27206390.0000 - mae: 27206390.0000 - val_loss: 23955860.0000 - val_mae: 23955860.0000\n",
      "Epoch 27/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994us/step - loss: 29707766.0000 - mae: 29707766.0000 - val_loss: 23571622.0000 - val_mae: 23571622.0000\n",
      "Epoch 28/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 25881884.0000 - mae: 25881884.0000 - val_loss: 23650148.0000 - val_mae: 23650148.0000\n",
      "Epoch 29/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 26355342.0000 - mae: 26355342.0000 - val_loss: 23523240.0000 - val_mae: 23523240.0000\n",
      "Epoch 30/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 24661702.0000 - mae: 24661702.0000 - val_loss: 23578424.0000 - val_mae: 23578424.0000\n",
      "Epoch 31/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 28315056.0000 - mae: 28315056.0000 - val_loss: 23644408.0000 - val_mae: 23644408.0000\n",
      "Epoch 32/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - loss: 26521042.0000 - mae: 26521042.0000 - val_loss: 23579050.0000 - val_mae: 23579050.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 25957410.0000 - mae: 25957410.0000 - val_loss: 23532482.0000 - val_mae: 23532482.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 27172988.0000 - mae: 27172988.0000 - val_loss: 23780580.0000 - val_mae: 23780580.0000\n",
      "Epoch 35/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950us/step - loss: 28043512.0000 - mae: 28043512.0000 - val_loss: 23661996.0000 - val_mae: 23661996.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - loss: 24516292.0000 - mae: 24516292.0000 - val_loss: 23406410.0000 - val_mae: 23406410.0000\n",
      "Epoch 37/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 25639258.0000 - mae: 25639258.0000 - val_loss: 23593948.0000 - val_mae: 23593948.0000\n",
      "Epoch 38/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 26178288.0000 - mae: 26178288.0000 - val_loss: 23460314.0000 - val_mae: 23460314.0000\n",
      "Epoch 39/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - loss: 26469600.0000 - mae: 26469600.0000 - val_loss: 23781054.0000 - val_mae: 23781054.0000\n",
      "Epoch 40/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 24497120.0000 - mae: 24497120.0000 - val_loss: 24108798.0000 - val_mae: 24108798.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - loss: 26985106.0000 - mae: 26985106.0000 - val_loss: 23496772.0000 - val_mae: 23496772.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 26492790.0000 - mae: 26492790.0000 - val_loss: 23605782.0000 - val_mae: 23605782.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 26987052.0000 - mae: 26987052.0000 - val_loss: 23520408.0000 - val_mae: 23520408.0000\n",
      "Epoch 44/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - loss: 24638356.0000 - mae: 24638356.0000 - val_loss: 23721520.0000 - val_mae: 23721520.0000\n",
      "Epoch 45/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 25922320.0000 - mae: 25922320.0000 - val_loss: 23875326.0000 - val_mae: 23875326.0000\n",
      "Epoch 46/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 25411998.0000 - mae: 25411998.0000 - val_loss: 23494660.0000 - val_mae: 23494660.0000\n",
      "Epoch 47/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 25411650.0000 - mae: 25411650.0000 - val_loss: 23894128.0000 - val_mae: 23894128.0000\n",
      "Epoch 48/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 24992670.0000 - mae: 24992670.0000 - val_loss: 23941768.0000 - val_mae: 23941768.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 25044756.0000 - mae: 25044756.0000 - val_loss: 23793052.0000 - val_mae: 23793052.0000\n",
      "Epoch 50/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 24003022.0000 - mae: 24003022.0000 - val_loss: 23942218.0000 - val_mae: 23942218.0000\n",
      "Epoch 51/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 23741060.0000 - mae: 23741060.0000 - val_loss: 23546746.0000 - val_mae: 23546746.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 23667330.0000 - mae: 23667330.0000 - val_loss: 23950720.0000 - val_mae: 23950720.0000\n",
      "Epoch 53/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 25693474.0000 - mae: 25693474.0000 - val_loss: 24038446.0000 - val_mae: 24038446.0000\n",
      "Epoch 54/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 25740104.0000 - mae: 25740104.0000 - val_loss: 23603026.0000 - val_mae: 23603026.0000\n",
      "Epoch 55/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 25736810.0000 - mae: 25736810.0000 - val_loss: 23856126.0000 - val_mae: 23856126.0000\n",
      "Epoch 56/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 25039480.0000 - mae: 25039480.0000 - val_loss: 23487654.0000 - val_mae: 23487654.0000\n",
      "Epoch 57/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - loss: 24734696.0000 - mae: 24734696.0000 - val_loss: 24168132.0000 - val_mae: 24168132.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 25938150.0000 - mae: 25938150.0000 - val_loss: 23986920.0000 - val_mae: 23986920.0000\n",
      "Epoch 59/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - loss: 24796144.0000 - mae: 24796144.0000 - val_loss: 23636328.0000 - val_mae: 23636328.0000\n",
      "Epoch 60/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - loss: 25110934.0000 - mae: 25110934.0000 - val_loss: 23798944.0000 - val_mae: 23798944.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - loss: 25295234.0000 - mae: 25295234.0000 - val_loss: 23740722.0000 - val_mae: 23740722.0000\n",
      "Epoch 62/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - loss: 25206940.0000 - mae: 25206940.0000 - val_loss: 23646160.0000 - val_mae: 23646160.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - loss: 25676554.0000 - mae: 25676554.0000 - val_loss: 23795974.0000 - val_mae: 23795974.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - loss: 25531500.0000 - mae: 25531500.0000 - val_loss: 24161780.0000 - val_mae: 24161780.0000\n",
      "Epoch 65/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 24644242.0000 - mae: 24644242.0000 - val_loss: 23917866.0000 - val_mae: 23917866.0000\n",
      "Epoch 66/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - loss: 24015828.0000 - mae: 24015828.0000 - val_loss: 23472586.0000 - val_mae: 23472586.0000\n",
      "Epoch 67/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 898us/step - loss: 24363146.0000 - mae: 24363146.0000 - val_loss: 23776216.0000 - val_mae: 23776216.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 24260950.0000 - mae: 24260950.0000 - val_loss: 23748958.0000 - val_mae: 23748958.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906us/step - loss: 24612076.0000 - mae: 24612076.0000 - val_loss: 24401476.0000 - val_mae: 24401476.0000\n",
      "Epoch 70/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - loss: 25073248.0000 - mae: 25073248.0000 - val_loss: 23749058.0000 - val_mae: 23749058.0000\n",
      "Epoch 71/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - loss: 25663812.0000 - mae: 25663812.0000 - val_loss: 23863370.0000 - val_mae: 23863370.0000\n",
      "Epoch 72/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - loss: 26677760.0000 - mae: 26677760.0000 - val_loss: 23507514.0000 - val_mae: 23507514.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - loss: 24199022.0000 - mae: 24199022.0000 - val_loss: 24491638.0000 - val_mae: 24491638.0000\n",
      "Epoch 74/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - loss: 25340822.0000 - mae: 25340822.0000 - val_loss: 24080500.0000 - val_mae: 24080500.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 25076038.0000 - mae: 25076038.0000 - val_loss: 23971116.0000 - val_mae: 23971116.0000\n",
      "Epoch 76/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - loss: 25162322.0000 - mae: 25162322.0000 - val_loss: 23770428.0000 - val_mae: 23770428.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - loss: 26218794.0000 - mae: 26218794.0000 - val_loss: 23491400.0000 - val_mae: 23491400.0000\n",
      "Epoch 78/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 25993692.0000 - mae: 25993692.0000 - val_loss: 23769046.0000 - val_mae: 23769046.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 26793864.0000 - mae: 26793864.0000 - val_loss: 23937804.0000 - val_mae: 23937804.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - loss: 24790932.0000 - mae: 24790932.0000 - val_loss: 23835548.0000 - val_mae: 23835548.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 26307394.0000 - mae: 26307394.0000 - val_loss: 24141842.0000 - val_mae: 24141842.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 25617966.0000 - mae: 25617966.0000 - val_loss: 23708764.0000 - val_mae: 23708764.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 24802790.0000 - mae: 24802790.0000 - val_loss: 23611520.0000 - val_mae: 23611520.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - loss: 23530536.0000 - mae: 23530536.0000 - val_loss: 23985280.0000 - val_mae: 23985280.0000\n",
      "Epoch 85/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896us/step - loss: 24721584.0000 - mae: 24721584.0000 - val_loss: 23884716.0000 - val_mae: 23884716.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 25558136.0000 - mae: 25558136.0000 - val_loss: 23600936.0000 - val_mae: 23600936.0000\n",
      "Epoch 87/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - loss: 24540686.0000 - mae: 24540686.0000 - val_loss: 23771196.0000 - val_mae: 23771196.0000\n",
      "Epoch 88/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 27814630.0000 - mae: 27814630.0000 - val_loss: 23738806.0000 - val_mae: 23738806.0000\n",
      "Epoch 89/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - loss: 25480628.0000 - mae: 25480628.0000 - val_loss: 23918794.0000 - val_mae: 23918794.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - loss: 24728130.0000 - mae: 24728130.0000 - val_loss: 23858216.0000 - val_mae: 23858216.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 23355358.0000 - mae: 23355358.0000 - val_loss: 23740508.0000 - val_mae: 23740508.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 25381904.0000 - mae: 25381904.0000 - val_loss: 24176602.0000 - val_mae: 24176602.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 24214396.0000 - mae: 24214396.0000 - val_loss: 24044916.0000 - val_mae: 24044916.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 23004054.0000 - mae: 23004054.0000 - val_loss: 23604118.0000 - val_mae: 23604118.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 26616006.0000 - mae: 26616006.0000 - val_loss: 24330250.0000 - val_mae: 24330250.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 26699520.0000 - mae: 26699520.0000 - val_loss: 23756722.0000 - val_mae: 23756722.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - loss: 24102870.0000 - mae: 24102870.0000 - val_loss: 23874654.0000 - val_mae: 23874654.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 25589254.0000 - mae: 25589254.0000 - val_loss: 23691510.0000 - val_mae: 23691510.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 25427674.0000 - mae: 25427674.0000 - val_loss: 24035802.0000 - val_mae: 24035802.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 24288260.0000 - mae: 24288260.0000 - val_loss: 23706212.0000 - val_mae: 23706212.0000\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - loss: 29171454.0000 - mae: 29171454.0000\n",
      "Mean Absolute Error on Test Set: 28157954.0\n"
     ]
    }
   ],
   "source": [
    "# también según Saturn cloud, aumentar el numero de neuronas tambien aumenta la precision (accuracy) del modelo\n",
    "# https://saturncloud.io/blog/how-to-improve-accuracy-in-neural-networks-with-keras/\n",
    "model3 = Sequential(\n",
    "    [\n",
    "        Dense(\n",
    "            units=128,\n",
    "            activation=\"relu\",\n",
    "            input_shape=(X_train_processed.shape[1],),\n",
    "            # input_dim=100,\n",
    "        ),\n",
    "        Dropout(0.2),\n",
    "        Dense(\n",
    "            units=64,\n",
    "            activation=\"relu\"\n",
    "        ),\n",
    "        Dropout(0.2),\n",
    "        Dense(\n",
    "            units=64,\n",
    "            activation=\"relu\"\n",
    "        ),\n",
    "        Dropout(0.2),\n",
    "        Dense(\n",
    "            units=64,\n",
    "            activation=\"relu\"\n",
    "        ),\n",
    "        Dense(1),  # Capa de salida para regresión\n",
    "    ]\n",
    ")\n",
    "\n",
    "model3.compile(\n",
    "    optimizer=Adam(learning_rate=0.004), loss=\"mean_absolute_error\", metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "history = model3.fit(\n",
    "    X_train_processed, y_train, epochs=100, batch_size=32, validation_split=0.2\n",
    ")\n",
    "loss, mae = model3.evaluate(X_test_processed, y_test)\n",
    "print(f\"Mean Absolute Error on Test Set: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 110316056.0000 - mae: 110316056.0000 - val_loss: 104878376.0000 - val_mae: 104878376.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 111402744.0000 - mae: 111402744.0000 - val_loss: 77357976.0000 - val_mae: 77357976.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 75154128.0000 - mae: 75154128.0000 - val_loss: 49153952.0000 - val_mae: 49153952.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 54973620.0000 - mae: 54973620.0000 - val_loss: 39421324.0000 - val_mae: 39421324.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 45241748.0000 - mae: 45241748.0000 - val_loss: 34577832.0000 - val_mae: 34577832.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 42529096.0000 - mae: 42529096.0000 - val_loss: 31867584.0000 - val_mae: 31867584.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 39362688.0000 - mae: 39362688.0000 - val_loss: 29708854.0000 - val_mae: 29708854.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 37253764.0000 - mae: 37253764.0000 - val_loss: 28786344.0000 - val_mae: 28786344.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 37137608.0000 - mae: 37137608.0000 - val_loss: 26934274.0000 - val_mae: 26934274.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 36504756.0000 - mae: 36504756.0000 - val_loss: 25872424.0000 - val_mae: 25872424.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 34040620.0000 - mae: 34040620.0000 - val_loss: 25210092.0000 - val_mae: 25210092.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 34130020.0000 - mae: 34130020.0000 - val_loss: 24632306.0000 - val_mae: 24632306.0000\n",
      "Epoch 13/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 34741256.0000 - mae: 34741256.0000 - val_loss: 24303040.0000 - val_mae: 24303040.0000\n",
      "Epoch 14/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - loss: 32264474.0000 - mae: 32264474.0000 - val_loss: 23994378.0000 - val_mae: 23994378.0000\n",
      "Epoch 15/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 33399772.0000 - mae: 33399772.0000 - val_loss: 23908224.0000 - val_mae: 23908224.0000\n",
      "Epoch 16/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 32604942.0000 - mae: 32604942.0000 - val_loss: 23664850.0000 - val_mae: 23664850.0000\n",
      "Epoch 17/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 32711624.0000 - mae: 32711624.0000 - val_loss: 24224548.0000 - val_mae: 24224548.0000\n",
      "Epoch 18/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 32306384.0000 - mae: 32306384.0000 - val_loss: 23477744.0000 - val_mae: 23477744.0000\n",
      "Epoch 19/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 33049802.0000 - mae: 33049802.0000 - val_loss: 23398164.0000 - val_mae: 23398164.0000\n",
      "Epoch 20/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 32570036.0000 - mae: 32570036.0000 - val_loss: 23610350.0000 - val_mae: 23610350.0000\n",
      "Epoch 21/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 34318496.0000 - mae: 34318496.0000 - val_loss: 23515926.0000 - val_mae: 23515926.0000\n",
      "Epoch 22/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 32036726.0000 - mae: 32036726.0000 - val_loss: 23423272.0000 - val_mae: 23423272.0000\n",
      "Epoch 23/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 30419876.0000 - mae: 30419876.0000 - val_loss: 23445214.0000 - val_mae: 23445214.0000\n",
      "Epoch 24/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 32131772.0000 - mae: 32131772.0000 - val_loss: 23437168.0000 - val_mae: 23437168.0000\n",
      "Epoch 25/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 30852864.0000 - mae: 30852864.0000 - val_loss: 23453752.0000 - val_mae: 23453752.0000\n",
      "Epoch 26/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 35087928.0000 - mae: 35087928.0000 - val_loss: 23278052.0000 - val_mae: 23278052.0000\n",
      "Epoch 27/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 33075556.0000 - mae: 33075556.0000 - val_loss: 23334800.0000 - val_mae: 23334800.0000\n",
      "Epoch 28/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 29419676.0000 - mae: 29419676.0000 - val_loss: 23495262.0000 - val_mae: 23495262.0000\n",
      "Epoch 29/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 30553264.0000 - mae: 30553264.0000 - val_loss: 23334930.0000 - val_mae: 23334930.0000\n",
      "Epoch 30/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 31219522.0000 - mae: 31219522.0000 - val_loss: 23605192.0000 - val_mae: 23605192.0000\n",
      "Epoch 31/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 31586906.0000 - mae: 31586906.0000 - val_loss: 23331708.0000 - val_mae: 23331708.0000\n",
      "Epoch 32/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 33697020.0000 - mae: 33697020.0000 - val_loss: 23277288.0000 - val_mae: 23277288.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 30442912.0000 - mae: 30442912.0000 - val_loss: 23150606.0000 - val_mae: 23150606.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 31005226.0000 - mae: 31005226.0000 - val_loss: 23222868.0000 - val_mae: 23222868.0000\n",
      "Epoch 35/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 32670552.0000 - mae: 32670552.0000 - val_loss: 23144820.0000 - val_mae: 23144820.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 31289668.0000 - mae: 31289668.0000 - val_loss: 23187486.0000 - val_mae: 23187486.0000\n",
      "Epoch 37/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 32658202.0000 - mae: 32658202.0000 - val_loss: 23281566.0000 - val_mae: 23281566.0000\n",
      "Epoch 38/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 33545060.0000 - mae: 33545060.0000 - val_loss: 23137968.0000 - val_mae: 23137968.0000\n",
      "Epoch 39/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 29765372.0000 - mae: 29765372.0000 - val_loss: 23672190.0000 - val_mae: 23672190.0000\n",
      "Epoch 40/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 33563504.0000 - mae: 33563504.0000 - val_loss: 23365576.0000 - val_mae: 23365576.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 31319046.0000 - mae: 31319046.0000 - val_loss: 23807126.0000 - val_mae: 23807126.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 31190878.0000 - mae: 31190878.0000 - val_loss: 23403074.0000 - val_mae: 23403074.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 31447302.0000 - mae: 31447302.0000 - val_loss: 23837758.0000 - val_mae: 23837758.0000\n",
      "Epoch 44/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 31805390.0000 - mae: 31805390.0000 - val_loss: 23280068.0000 - val_mae: 23280068.0000\n",
      "Epoch 45/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 31109610.0000 - mae: 31109610.0000 - val_loss: 23354458.0000 - val_mae: 23354458.0000\n",
      "Epoch 46/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 34224480.0000 - mae: 34224480.0000 - val_loss: 23297476.0000 - val_mae: 23297476.0000\n",
      "Epoch 47/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 31809382.0000 - mae: 31809382.0000 - val_loss: 23226544.0000 - val_mae: 23226544.0000\n",
      "Epoch 48/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 31352714.0000 - mae: 31352714.0000 - val_loss: 23405190.0000 - val_mae: 23405190.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - loss: 29977560.0000 - mae: 29977560.0000 - val_loss: 23151440.0000 - val_mae: 23151440.0000\n",
      "Epoch 50/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 32795446.0000 - mae: 32795446.0000 - val_loss: 23250942.0000 - val_mae: 23250942.0000\n",
      "Epoch 51/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 32236254.0000 - mae: 32236254.0000 - val_loss: 23377596.0000 - val_mae: 23377596.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 31498080.0000 - mae: 31498080.0000 - val_loss: 23290422.0000 - val_mae: 23290422.0000\n",
      "Epoch 53/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 31622646.0000 - mae: 31622646.0000 - val_loss: 23777040.0000 - val_mae: 23777040.0000\n",
      "Epoch 54/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 31322364.0000 - mae: 31322364.0000 - val_loss: 23390510.0000 - val_mae: 23390510.0000\n",
      "Epoch 55/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 32099678.0000 - mae: 32099678.0000 - val_loss: 23457134.0000 - val_mae: 23457134.0000\n",
      "Epoch 56/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 31441422.0000 - mae: 31441422.0000 - val_loss: 23465060.0000 - val_mae: 23465060.0000\n",
      "Epoch 57/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 35299640.0000 - mae: 35299640.0000 - val_loss: 23591376.0000 - val_mae: 23591376.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 29858200.0000 - mae: 29858200.0000 - val_loss: 23517280.0000 - val_mae: 23517280.0000\n",
      "Epoch 59/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 28919792.0000 - mae: 28919792.0000 - val_loss: 23384014.0000 - val_mae: 23384014.0000\n",
      "Epoch 60/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 30744908.0000 - mae: 30744908.0000 - val_loss: 23361260.0000 - val_mae: 23361260.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - loss: 33691260.0000 - mae: 33691260.0000 - val_loss: 23263606.0000 - val_mae: 23263606.0000\n",
      "Epoch 62/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - loss: 31569368.0000 - mae: 31569368.0000 - val_loss: 23499188.0000 - val_mae: 23499188.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 30810370.0000 - mae: 30810370.0000 - val_loss: 23484216.0000 - val_mae: 23484216.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 30158120.0000 - mae: 30158120.0000 - val_loss: 23448660.0000 - val_mae: 23448660.0000\n",
      "Epoch 65/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 31950216.0000 - mae: 31950216.0000 - val_loss: 23375642.0000 - val_mae: 23375642.0000\n",
      "Epoch 66/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 30443720.0000 - mae: 30443720.0000 - val_loss: 23483734.0000 - val_mae: 23483734.0000\n",
      "Epoch 67/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 30624074.0000 - mae: 30624074.0000 - val_loss: 23517554.0000 - val_mae: 23517554.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - loss: 32197074.0000 - mae: 32197074.0000 - val_loss: 23518938.0000 - val_mae: 23518938.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 31353076.0000 - mae: 31353076.0000 - val_loss: 23452736.0000 - val_mae: 23452736.0000\n",
      "Epoch 70/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 32182538.0000 - mae: 32182538.0000 - val_loss: 23487424.0000 - val_mae: 23487424.0000\n",
      "Epoch 71/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 33752096.0000 - mae: 33752096.0000 - val_loss: 23535114.0000 - val_mae: 23535114.0000\n",
      "Epoch 72/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 29433022.0000 - mae: 29433022.0000 - val_loss: 23417874.0000 - val_mae: 23417874.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 31281116.0000 - mae: 31281116.0000 - val_loss: 23517888.0000 - val_mae: 23517888.0000\n",
      "Epoch 74/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 33985600.0000 - mae: 33985600.0000 - val_loss: 23747498.0000 - val_mae: 23747498.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 30013876.0000 - mae: 30013876.0000 - val_loss: 23659666.0000 - val_mae: 23659666.0000\n",
      "Epoch 76/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 30088742.0000 - mae: 30088742.0000 - val_loss: 23715662.0000 - val_mae: 23715662.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 31701534.0000 - mae: 31701534.0000 - val_loss: 23507502.0000 - val_mae: 23507502.0000\n",
      "Epoch 78/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 32271630.0000 - mae: 32271630.0000 - val_loss: 23475912.0000 - val_mae: 23475912.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 34594580.0000 - mae: 34594580.0000 - val_loss: 23717110.0000 - val_mae: 23717110.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - loss: 29147970.0000 - mae: 29147970.0000 - val_loss: 23713192.0000 - val_mae: 23713192.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 32345932.0000 - mae: 32345932.0000 - val_loss: 23633880.0000 - val_mae: 23633880.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 31680380.0000 - mae: 31680380.0000 - val_loss: 23630942.0000 - val_mae: 23630942.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 30118714.0000 - mae: 30118714.0000 - val_loss: 24060680.0000 - val_mae: 24060680.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 31399914.0000 - mae: 31399914.0000 - val_loss: 23569260.0000 - val_mae: 23569260.0000\n",
      "Epoch 85/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 31490144.0000 - mae: 31490144.0000 - val_loss: 23521298.0000 - val_mae: 23521298.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 30150410.0000 - mae: 30150410.0000 - val_loss: 23417874.0000 - val_mae: 23417874.0000\n",
      "Epoch 87/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 28755714.0000 - mae: 28755714.0000 - val_loss: 23290736.0000 - val_mae: 23290736.0000\n",
      "Epoch 88/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - loss: 29986900.0000 - mae: 29986900.0000 - val_loss: 23490398.0000 - val_mae: 23490398.0000\n",
      "Epoch 89/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 31459598.0000 - mae: 31459598.0000 - val_loss: 23680066.0000 - val_mae: 23680066.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 31501424.0000 - mae: 31501424.0000 - val_loss: 23386120.0000 - val_mae: 23386120.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 31892248.0000 - mae: 31892248.0000 - val_loss: 23555796.0000 - val_mae: 23555796.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 32376702.0000 - mae: 32376702.0000 - val_loss: 23454846.0000 - val_mae: 23454846.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 32577618.0000 - mae: 32577618.0000 - val_loss: 23455108.0000 - val_mae: 23455108.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 32398440.0000 - mae: 32398440.0000 - val_loss: 23387692.0000 - val_mae: 23387692.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 31107564.0000 - mae: 31107564.0000 - val_loss: 23671614.0000 - val_mae: 23671614.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 32865724.0000 - mae: 32865724.0000 - val_loss: 23382868.0000 - val_mae: 23382868.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 32269536.0000 - mae: 32269536.0000 - val_loss: 23376974.0000 - val_mae: 23376974.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 29918414.0000 - mae: 29918414.0000 - val_loss: 23451284.0000 - val_mae: 23451284.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 28927436.0000 - mae: 28927436.0000 - val_loss: 23342450.0000 - val_mae: 23342450.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 32481232.0000 - mae: 32481232.0000 - val_loss: 24136614.0000 - val_mae: 24136614.0000\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 28996894.0000 - mae: 28996894.0000\n",
      "Mean Absolute Error on Test Set: 28148588.0\n"
     ]
    }
   ],
   "source": [
    "# según Saturn cloud, el valor de Dropout de 0.5 provee un mayor accuracy\n",
    "# https://saturncloud.io/blog/how-to-improve-accuracy-in-neural-networks-with-keras/\n",
    "model4 = Sequential(\n",
    "    [\n",
    "        Dense(\n",
    "            units=64,\n",
    "            activation=\"relu\",\n",
    "            input_shape=(X_train_processed.shape[1],),\n",
    "        ),\n",
    "        Dropout(0.5),\n",
    "        Dense(\n",
    "            units=64,\n",
    "            activation=\"relu\"\n",
    "        ),\n",
    "        Dropout(0.5),\n",
    "        Dense(\n",
    "            units=64,\n",
    "            activation=\"relu\"\n",
    "        ),\n",
    "        Dropout(0.5),\n",
    "        Dense(\n",
    "            units=64,\n",
    "            activation=\"relu\"\n",
    "        ),\n",
    "        Dense(1),  # Capa de salida para regresión\n",
    "    ]\n",
    ")\n",
    "\n",
    "model4.compile(\n",
    "    optimizer=Adam(learning_rate=0.004), loss=\"mean_absolute_error\", metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "history = model4.fit(\n",
    "    X_train_processed, y_train, epochs=100, batch_size=32, validation_split=0.2\n",
    ")\n",
    "loss, mae = model4.evaluate(X_test_processed, y_test)\n",
    "print(f\"Mean Absolute Error on Test Set: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 591us/step - loss: 113112536.0000 - mae: 113112536.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 95814136.0000 - mae: 95814136.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490us/step - loss: 59921360.0000 - mae: 59921360.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step - loss: 42673152.0000 - mae: 42673152.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 41800656.0000 - mae: 41800656.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487us/step - loss: 37166328.0000 - mae: 37166328.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - loss: 34415836.0000 - mae: 34415836.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step - loss: 36316580.0000 - mae: 36316580.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 33815352.0000 - mae: 33815352.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - loss: 32932620.0000 - mae: 32932620.0000\n",
      "Epoch 1/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34600216.0000 - mae: 34600216.0000 - val_loss: 23245416.0000 - val_mae: 23245416.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 34059596.0000 - mae: 34059596.0000 - val_loss: 23175242.0000 - val_mae: 23175242.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 33570092.0000 - mae: 33570092.0000 - val_loss: 23004948.0000 - val_mae: 23004948.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 33110648.0000 - mae: 33110648.0000 - val_loss: 23064226.0000 - val_mae: 23064226.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - loss: 35930444.0000 - mae: 35930444.0000 - val_loss: 23008512.0000 - val_mae: 23008512.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 30968462.0000 - mae: 30968462.0000 - val_loss: 23464240.0000 - val_mae: 23464240.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 33953736.0000 - mae: 33953736.0000 - val_loss: 22834802.0000 - val_mae: 22834802.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 31600476.0000 - mae: 31600476.0000 - val_loss: 23238020.0000 - val_mae: 23238020.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 30722402.0000 - mae: 30722402.0000 - val_loss: 22932534.0000 - val_mae: 22932534.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 32137014.0000 - mae: 32137014.0000 - val_loss: 22988256.0000 - val_mae: 22988256.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 33632504.0000 - mae: 33632504.0000 - val_loss: 23104634.0000 - val_mae: 23104634.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 31418366.0000 - mae: 31418366.0000 - val_loss: 22795770.0000 - val_mae: 22795770.0000\n",
      "Epoch 13/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 30656870.0000 - mae: 30656870.0000 - val_loss: 23285130.0000 - val_mae: 23285130.0000\n",
      "Epoch 14/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 34840432.0000 - mae: 34840432.0000 - val_loss: 23046302.0000 - val_mae: 23046302.0000\n",
      "Epoch 15/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 33343232.0000 - mae: 33343232.0000 - val_loss: 23388362.0000 - val_mae: 23388362.0000\n",
      "Epoch 16/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 33043906.0000 - mae: 33043906.0000 - val_loss: 23042924.0000 - val_mae: 23042924.0000\n",
      "Epoch 17/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 31487618.0000 - mae: 31487618.0000 - val_loss: 23193506.0000 - val_mae: 23193506.0000\n",
      "Epoch 18/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 32591918.0000 - mae: 32591918.0000 - val_loss: 23062394.0000 - val_mae: 23062394.0000\n",
      "Epoch 19/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 32725898.0000 - mae: 32725898.0000 - val_loss: 22955950.0000 - val_mae: 22955950.0000\n",
      "Epoch 20/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 32788722.0000 - mae: 32788722.0000 - val_loss: 23136612.0000 - val_mae: 23136612.0000\n",
      "Epoch 21/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 32266774.0000 - mae: 32266774.0000 - val_loss: 23113070.0000 - val_mae: 23113070.0000\n",
      "Epoch 22/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 32912712.0000 - mae: 32912712.0000 - val_loss: 23220628.0000 - val_mae: 23220628.0000\n",
      "Epoch 23/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 31469194.0000 - mae: 31469194.0000 - val_loss: 23315318.0000 - val_mae: 23315318.0000\n",
      "Epoch 24/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 29379522.0000 - mae: 29379522.0000 - val_loss: 23180984.0000 - val_mae: 23180984.0000\n",
      "Epoch 25/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 30233498.0000 - mae: 30233498.0000 - val_loss: 23102872.0000 - val_mae: 23102872.0000\n",
      "Epoch 26/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 35428084.0000 - mae: 35428084.0000 - val_loss: 23020714.0000 - val_mae: 23020714.0000\n",
      "Epoch 27/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 31368592.0000 - mae: 31368592.0000 - val_loss: 23190660.0000 - val_mae: 23190660.0000\n",
      "Epoch 28/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 31598840.0000 - mae: 31598840.0000 - val_loss: 23222020.0000 - val_mae: 23222020.0000\n",
      "Epoch 29/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 30048644.0000 - mae: 30048644.0000 - val_loss: 23527146.0000 - val_mae: 23527146.0000\n",
      "Epoch 30/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 33231998.0000 - mae: 33231998.0000 - val_loss: 23472036.0000 - val_mae: 23472036.0000\n",
      "Epoch 31/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 33651484.0000 - mae: 33651484.0000 - val_loss: 23213676.0000 - val_mae: 23213676.0000\n",
      "Epoch 32/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 32255366.0000 - mae: 32255366.0000 - val_loss: 23096946.0000 - val_mae: 23096946.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 30331358.0000 - mae: 30331358.0000 - val_loss: 23106790.0000 - val_mae: 23106790.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 32960072.0000 - mae: 32960072.0000 - val_loss: 23083808.0000 - val_mae: 23083808.0000\n",
      "Epoch 35/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 31624822.0000 - mae: 31624822.0000 - val_loss: 22831436.0000 - val_mae: 22831436.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 31227466.0000 - mae: 31227466.0000 - val_loss: 23238258.0000 - val_mae: 23238258.0000\n",
      "Epoch 37/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 32468316.0000 - mae: 32468316.0000 - val_loss: 23085904.0000 - val_mae: 23085904.0000\n",
      "Epoch 38/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 30137436.0000 - mae: 30137436.0000 - val_loss: 23232710.0000 - val_mae: 23232710.0000\n",
      "Epoch 39/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 32479618.0000 - mae: 32479618.0000 - val_loss: 23173260.0000 - val_mae: 23173260.0000\n",
      "Epoch 40/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 31816624.0000 - mae: 31816624.0000 - val_loss: 23212126.0000 - val_mae: 23212126.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 30932968.0000 - mae: 30932968.0000 - val_loss: 23060136.0000 - val_mae: 23060136.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 32104976.0000 - mae: 32104976.0000 - val_loss: 23103794.0000 - val_mae: 23103794.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 32208500.0000 - mae: 32208500.0000 - val_loss: 23128072.0000 - val_mae: 23128072.0000\n",
      "Epoch 44/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 31124368.0000 - mae: 31124368.0000 - val_loss: 23213784.0000 - val_mae: 23213784.0000\n",
      "Epoch 45/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 31125770.0000 - mae: 31125770.0000 - val_loss: 23405014.0000 - val_mae: 23405014.0000\n",
      "Epoch 46/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 31599028.0000 - mae: 31599028.0000 - val_loss: 22994076.0000 - val_mae: 22994076.0000\n",
      "Epoch 47/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 32870412.0000 - mae: 32870412.0000 - val_loss: 23139050.0000 - val_mae: 23139050.0000\n",
      "Epoch 48/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 32160070.0000 - mae: 32160070.0000 - val_loss: 23164858.0000 - val_mae: 23164858.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 31985604.0000 - mae: 31985604.0000 - val_loss: 23693540.0000 - val_mae: 23693540.0000\n",
      "Epoch 50/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 27376810.0000 - mae: 27376810.0000 - val_loss: 23679904.0000 - val_mae: 23679904.0000\n",
      "Epoch 51/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 32374346.0000 - mae: 32374346.0000 - val_loss: 23099672.0000 - val_mae: 23099672.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 31664758.0000 - mae: 31664758.0000 - val_loss: 23161358.0000 - val_mae: 23161358.0000\n",
      "Epoch 53/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 33599720.0000 - mae: 33599720.0000 - val_loss: 23409890.0000 - val_mae: 23409890.0000\n",
      "Epoch 54/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 32251804.0000 - mae: 32251804.0000 - val_loss: 23471970.0000 - val_mae: 23471970.0000\n",
      "Epoch 55/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 31212456.0000 - mae: 31212456.0000 - val_loss: 23001762.0000 - val_mae: 23001762.0000\n",
      "Epoch 56/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 29989104.0000 - mae: 29989104.0000 - val_loss: 23483286.0000 - val_mae: 23483286.0000\n",
      "Epoch 57/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 33024884.0000 - mae: 33024884.0000 - val_loss: 23053644.0000 - val_mae: 23053644.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 31938848.0000 - mae: 31938848.0000 - val_loss: 23366760.0000 - val_mae: 23366760.0000\n",
      "Epoch 59/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 31926786.0000 - mae: 31926786.0000 - val_loss: 23022752.0000 - val_mae: 23022752.0000\n",
      "Epoch 60/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 30963420.0000 - mae: 30963420.0000 - val_loss: 23191182.0000 - val_mae: 23191182.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 31102672.0000 - mae: 31102672.0000 - val_loss: 23362860.0000 - val_mae: 23362860.0000\n",
      "Epoch 62/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 30164220.0000 - mae: 30164220.0000 - val_loss: 23358794.0000 - val_mae: 23358794.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 31892476.0000 - mae: 31892476.0000 - val_loss: 23099600.0000 - val_mae: 23099600.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 33571392.0000 - mae: 33571392.0000 - val_loss: 23256164.0000 - val_mae: 23256164.0000\n",
      "Epoch 65/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 32703070.0000 - mae: 32703070.0000 - val_loss: 23004406.0000 - val_mae: 23004406.0000\n",
      "Epoch 66/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 30120490.0000 - mae: 30120490.0000 - val_loss: 23362258.0000 - val_mae: 23362258.0000\n",
      "Epoch 67/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 32994708.0000 - mae: 32994708.0000 - val_loss: 23125204.0000 - val_mae: 23125204.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 28382144.0000 - mae: 28382144.0000 - val_loss: 23370092.0000 - val_mae: 23370092.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 31972508.0000 - mae: 31972508.0000 - val_loss: 23219248.0000 - val_mae: 23219248.0000\n",
      "Epoch 70/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 30861196.0000 - mae: 30861196.0000 - val_loss: 23324860.0000 - val_mae: 23324860.0000\n",
      "Epoch 71/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 32530844.0000 - mae: 32530844.0000 - val_loss: 23239640.0000 - val_mae: 23239640.0000\n",
      "Epoch 72/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 31667160.0000 - mae: 31667160.0000 - val_loss: 23330178.0000 - val_mae: 23330178.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 31246880.0000 - mae: 31246880.0000 - val_loss: 23506356.0000 - val_mae: 23506356.0000\n",
      "Epoch 74/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 30247700.0000 - mae: 30247700.0000 - val_loss: 23355698.0000 - val_mae: 23355698.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 32003372.0000 - mae: 32003372.0000 - val_loss: 23355202.0000 - val_mae: 23355202.0000\n",
      "Epoch 76/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 31494900.0000 - mae: 31494900.0000 - val_loss: 23340760.0000 - val_mae: 23340760.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 32067164.0000 - mae: 32067164.0000 - val_loss: 23213066.0000 - val_mae: 23213066.0000\n",
      "Epoch 78/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 29310442.0000 - mae: 29310442.0000 - val_loss: 23215020.0000 - val_mae: 23215020.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 29782490.0000 - mae: 29782490.0000 - val_loss: 23256162.0000 - val_mae: 23256162.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 32040898.0000 - mae: 32040898.0000 - val_loss: 23388320.0000 - val_mae: 23388320.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 31494894.0000 - mae: 31494894.0000 - val_loss: 23403706.0000 - val_mae: 23403706.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 30531928.0000 - mae: 30531928.0000 - val_loss: 23348454.0000 - val_mae: 23348454.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 29449914.0000 - mae: 29449914.0000 - val_loss: 23323772.0000 - val_mae: 23323772.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 30469016.0000 - mae: 30469016.0000 - val_loss: 23364512.0000 - val_mae: 23364512.0000\n",
      "Epoch 85/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 32010334.0000 - mae: 32010334.0000 - val_loss: 23640364.0000 - val_mae: 23640364.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 33037328.0000 - mae: 33037328.0000 - val_loss: 23599618.0000 - val_mae: 23599618.0000\n",
      "Epoch 87/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 31869538.0000 - mae: 31869538.0000 - val_loss: 23271456.0000 - val_mae: 23271456.0000\n",
      "Epoch 88/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 30654826.0000 - mae: 30654826.0000 - val_loss: 23402824.0000 - val_mae: 23402824.0000\n",
      "Epoch 89/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 30959814.0000 - mae: 30959814.0000 - val_loss: 23234968.0000 - val_mae: 23234968.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 31937434.0000 - mae: 31937434.0000 - val_loss: 23231262.0000 - val_mae: 23231262.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 28983916.0000 - mae: 28983916.0000 - val_loss: 23262466.0000 - val_mae: 23262466.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 30650074.0000 - mae: 30650074.0000 - val_loss: 23305328.0000 - val_mae: 23305328.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 32592510.0000 - mae: 32592510.0000 - val_loss: 23342472.0000 - val_mae: 23342472.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 30852872.0000 - mae: 30852872.0000 - val_loss: 23254760.0000 - val_mae: 23254760.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 35144796.0000 - mae: 35144796.0000 - val_loss: 23281930.0000 - val_mae: 23281930.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 31286568.0000 - mae: 31286568.0000 - val_loss: 23395374.0000 - val_mae: 23395374.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 31064536.0000 - mae: 31064536.0000 - val_loss: 23449908.0000 - val_mae: 23449908.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 31433118.0000 - mae: 31433118.0000 - val_loss: 23518340.0000 - val_mae: 23518340.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 31818176.0000 - mae: 31818176.0000 - val_loss: 23261728.0000 - val_mae: 23261728.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 30416414.0000 - mae: 30416414.0000 - val_loss: 23510422.0000 - val_mae: 23510422.0000\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - loss: 28987760.0000 - mae: 28987760.0000\n",
      "Mean Absolute Error on Test Set: 27899092.0\n"
     ]
    }
   ],
   "source": [
    "# según Saturn cloud, aumentar el numero de Epochs también ayuda a aumentar la precision (accuracy) del modelo\n",
    "# https://saturncloud.io/blog/how-to-improve-accuracy-in-neural-networks-with-keras/\n",
    "model5 = Sequential(\n",
    "    [\n",
    "        Dense(\n",
    "            units=64,\n",
    "            activation=\"relu\",\n",
    "            input_shape=(X_train_processed.shape[1],),\n",
    "        ),\n",
    "        Dropout(0.5),\n",
    "        Dense(\n",
    "            units=64,\n",
    "            activation=\"relu\"\n",
    "        ),\n",
    "        Dropout(0.5),\n",
    "        Dense(\n",
    "            units=64,\n",
    "            activation=\"relu\"\n",
    "        ),\n",
    "        Dropout(0.5),\n",
    "        Dense(\n",
    "            units=64,\n",
    "            activation=\"relu\"\n",
    "        ),\n",
    "        Dense(1),  # Capa de salida para regresión\n",
    "    ]\n",
    ")\n",
    "\n",
    "# One-Hot Encoding for categorical features\n",
    "categorical_features = [\"genre_1\", \"genre_2\", \"genre_3\"]\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "model5.compile(\n",
    "    optimizer=Adam(learning_rate=0.004), loss=\"mean_absolute_error\", metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "model5.fit(X_train_processed, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "history = model5.fit(\n",
    "    X_train_processed, y_train, epochs=100, batch_size=32, validation_split=0.2\n",
    ")\n",
    "loss, mae = model5.evaluate(X_test_processed, y_test)\n",
    "print(f\"Mean Absolute Error on Test Set: {mae}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
